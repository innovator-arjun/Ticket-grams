{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'not'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a23a75e3edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n\\n# coding: utf-8\\n\\n# In[2]:\\n\\nimport os\\nimport pandas as pd\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.util import ngrams\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport seaborn as sns\\nfrom string import punctuation\\nfrom nltk.tokenize import RegexpTokenizer\\nimport re\\nimport collections\\nfrom past.builtins import xrange\\nimport spacy\\nnlp=spacy.load(\\'en_core_web_sm\\')\\n\\n\\n# In[3]:\\n\\n#pointing the source and listing the file names under that path\\npath = os.chdir(\\'C://Users//ar393556//Documents//Ticket grams//sample\\')\\nfiles = os.listdir(path)\\nfiles\\n\\n\\n#Fetching only xlsx files\\nfiles_xls = [f for f in files if f[-4:] == \\'xlsx\\']\\nfiles_xls\\n\\n\\n# In[5]:\\n\\ndf = pd.DataFrame()\\n\\n#Reading the files and appending the data in the DataFrame format\\nfor f in files_xls:\\n    data = pd.read_excel(f,sheet_name=\\'CI-Data\\')\\n    df = df.append(data)\\n\\n\\n# In[6]:\\n\\n#Taking only TicketID and Summary which are essential\\n\\nticket_df_full = pd.DataFrame(df, columns= [\\'TicketID\\',\\'Summary\\'])\\n\\nticket_df=ticket_df_full[[\\'TicketID\\',\\'Summary\\']]\\n\\n#no. of records and columns\\nticket_df.shape\\n\\n#Dropping null/blank records\\nticket_df=ticket_df.dropna()\\n\\n\\n# In[7]:\\n\\n#New table data shape\\nticket_df.shape\\n\\n\\n# In[8]:\\n\\nticket_df.head()\\n\\n\\n# In[9]:\\n\\nticket_df[\\'unclean\\']=ticket_df[\\'Summary\\']\\n\\n\\n# In[10]:\\n\\nticket_df.head()\\n\\n\\n# In[11]:\\n\\n# To remove punchuation, numbers and converting the string to lower\\nticket_df[\\'Summary\\'] = ticket_df[\\'Summary\\'].str.lower().str.replace(r\\'[^a-z\\\\s]\\', \\'\\')\\n\\n\\n# In[12]:\\n\\nticket_df.head()    \\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[13]:\\n\\nnlp.Defaults.stop_words.add(\\'thanks\\')\\nnlp.Defaults.stop_words.add(\\'please\\')\\nnlp.Defaults.stop_words.add(\\'team\\')\\nnlp.Defaults.stop_words.add(\\'dear\\')\\nnlp.Defaults.stop_words.add(\\'hi\\')\\n\\n\\n# In[14]:\\n\\nnlp.Defaults.stop_words.remove(\\'not\\')\\nnlp.Defaults.stop_words.remove(\\'cannot\\')\\nnlp.Defaults.stop_words.remove(\\'nothing\\')\\n\\n\\n# In[15]:\\n\\nstop_words=nlp.Defaults.stop_words\\n\\n\\n# In[16]:\\n\\n\\n\\nticket_df[\\'Summary\\'] = ticket_df[\\'Summary\\'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\\n\\n\\n# In[17]:\\n\\nticket_df[\\'Summary\\'] = ticket_df[\\'Summary\\'].apply(lambda x: \" \".join(x for x in x.split() if len(x)>=2 and len(x)<15))\\n\\n\\n# In[18]:\\nticket_df[\\'Summary\\']= ticket_df[\\'Summary\\'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\\n# In[19]:\\n\\nticket_df.head()\\n\\n\\n# In[20]:\\n\\n\\n\\ndef get_ngrams(text, n ):\\n    n_grams = ngrams(word_tokenize(text), n)\\n    return [ \\' \\'.join(grams) for grams in n_grams]\\n\\n\\n# In[21]:\\n\\n# ticket_df.to_csv(\\'final words.csv\\')\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[22]:\\n\\n\\nui=collections.Counter()\\n\\nfor i in ticket_df[\\'Summary\\']:\\n    x = i.rstrip().split(\" \")\\n    ui.update(ngrams(x, n=1))\\n\\n\\n# In[23]:\\n\\nui\\n\\n\\n# In[24]:\\n\\nui_counter = pd.DataFrame.from_dict(ui, orient=\\'index\\')\\nui_counter.to_csv(\\'uigrams.csv\\')\\nui_df=pd.read_csv(\\'uigrams.csv\\',index_col=False)\\nui_df[\\'Unnamed: 0\\'] = ui_df[\\'Unnamed: 0\\'].str.lower().str.replace(r\\'[^a-z\\\\s]\\', \\'\\')\\ndict_ui=ui_df.set_index(\\'Unnamed: 0\\')[\\'0\\'].to_dict()\\n\\n\\n# In[25]:\\n\\ndict_ui\\n\\n\\n# In[26]:\\n\\n\\nbi=collections.Counter()\\nfor i in ticket_df[\\'Summary\\']:\\n    x = i.rstrip().split(\" \")\\n    bi.update(set(zip(x[:-1],x[1:])))\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[27]:\\n\\nbigram_counter = pd.DataFrame.from_dict(bi, orient=\\'index\\')\\nbigram_counter.to_csv(\\'bigrams.csv\\')\\nbigram_df=pd.read_csv(\\'bigrams.csv\\',index_col=False)\\nbigram_df[\\'Unnamed: 0\\'] = bigram_df[\\'Unnamed: 0\\'].str.lower().str.replace(r\\'[^a-z\\\\s]\\', \\'\\')\\ndict_bigram=bigram_df.set_index(\\'Unnamed: 0\\')[\\'0\\'].to_dict()\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[28]:\\n\\n\\ntri=collections.Counter()\\nfor i in ticket_df[\\'Summary\\']:\\n    x = i.rstrip().split(\" \")\\n    tri.update(set(zip(x[:-2],x[1:-1],x[2:])))\\n\\n\\n# In[29]:\\n\\ntrigram_counter = pd.DataFrame.from_dict(tri, orient=\\'index\\')\\ntrigram_counter.to_csv(\\'trigram.csv\\')\\n\\n\\n# In[30]:\\n\\ntrigram_df=pd.read_csv(\\'trigram.csv\\',index_col=False)\\ntrigram_df[\\'Unnamed: 0\\'] = trigram_df[\\'Unnamed: 0\\'].str.lower().str.replace(r\\'[^a-z\\\\s]\\', \\'\\')\\ndict_trigram=trigram_df.set_index(\\'Unnamed: 0\\')[\\'0\\'].to_dict()\\n\\n\\n# In[31]:\\n\\n\\nfour=collections.Counter()\\n\\nfor i in ticket_df[\\'Summary\\']:\\n    x = i.rstrip().split(\" \")\\n    four.update(ngrams(x, n=4))\\n\\n\\n# In[32]:\\n\\nfourgram_counter = pd.DataFrame.from_dict(four, orient=\\'index\\')\\nfourgram_counter.to_csv(\\'fourgram.csv\\')\\nfourgram_df=pd.read_csv(\\'fourgram.csv\\',index_col=False)\\nfourgram_df[\\'Unnamed: 0\\'] = fourgram_df[\\'Unnamed: 0\\'].str.lower().str.replace(r\\'[^a-z\\\\s]\\', \\'\\')\\ndict_four=fourgram_df.set_index(\\'Unnamed: 0\\')[\\'0\\'].to_dict()\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[33]:\\n\\n\\ndef top_grams(dicti, N):\\n    res_topgram=[]\\n    for text in ticket_df[\\'Summary\\']:  \\n\\n        comp_list=get_ngrams(text,N)\\n        n = {k: dicti[k] for k in comp_list if k in dicti}\\n        flag=bool(n)\\n        res=\\'\\'\\n        if flag==True:\\n            res=max(n, key=n.get)\\n        else:\\n            res=\\'No Pattern\\'\\n        res_topgram.append(res)\\n        \\n    return res_topgram\\n\\n\\n# In[34]:\\n\\ndict_ui\\n\\n\\n# In[35]:\\n\\n\\nres_bigram=top_grams(dict_ui,1)\\nticket_df[\\'Top1Grams\\']=pd.DataFrame({\\'Top1Grams\\':res_bigram})\\n\\n\\n# In[36]:\\n\\nres_bigram\\n\\n\\n# In[37]:\\n\\n\\nres_bigram=top_grams(dict_bigram,2)\\nticket_df[\\'Top2Grams\\']=pd.DataFrame({\\'Top2Grams\\':res_bigram})\\n\\n\\n# In[38]:\\n\\nticket_df.head()\\n\\n\\n# In[39]:\\n\\nres_trigram=top_grams(dict_trigram, 3)\\nticket_df[\\'Top3Grams\\']=pd.DataFrame({\\'Top3Grams\\':res_trigram})\\n\\n\\n# In[40]:\\n\\nticket_df.head()\\n\\n\\n# In[41]:\\n\\n\\nres_fourgram=top_grams(dict_four,4)\\nticket_df[\\'Top4Grams\\']=pd.DataFrame({\\'Top4Grams\\':res_fourgram})\\n\\n\\n# In[42]:\\n\\nticket_df.tail()\\n\\n\\n# In[43]:\\n\\nticket_df.to_csv(\\'ticket_df_with_topgrams.csv\\')\\n\\n\\n# In[44]:\\n\\npath = os.chdir(\\'C://Users//ar393556//Documents//Ticket grams//CIKeyword\\')\\nfiles = os.listdir(path)\\n\\nautomation_df=pd.read_excel(\\'CIKeywords.xlsx\\')\\n\\n\\n# In[45]:\\n\\nautomation_df.head()\\n\\n\\n# In[46]:\\n\\nautomation_df[\\'Keyword\\']=automation_df[\\'Keyword\\'].apply(lambda x: x.strip(punctuation))\\nautomation_df[\\'Keyword\\']=automation_df[\\'Keyword\\'].str.replace(\\'*\\',\\' \\')\\n\\n\\n# In[47]:\\n\\n# To remove punchuation, numbers and converting the string to lower\\nautomation_df[\\'Keyword\\']=automation_df[\\'Keyword\\'].str.lower()\\n\\n\\n# In[48]:\\n\\nautomation_df[\\'Keyword\\']=automation_df[\\'Keyword\\'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\\n\\n# In[49]:\\n\\nautomation_df.head()\\n\\n\\n# In[50]:\\n\\ndict_keyword=automation_df.set_index(\\'Keyword\\')[\\'Weight\\'].to_dict()\\n\\n\\n# In[51]:\\n\\ndict_automation_text=automation_df.set_index(\\'Keyword\\')[\\'AutomationCategory\\'].to_dict()\\n\\n\\n# In[52]:\\n\\nlen(ticket_df)\\nfrom fuzzywuzzy import process\\nfrom fuzzywuzzy import fuzz\\n\\n\\n# In[53]:\\nautomation_col=[]\\n# for i in columns:\\nfor record in range(0,len(ticket_df)):\\n    \\n    grams_list_record=[]\\n    grams_list_record.append(ticket_df[\\'Top1Grams\\'][record])\\n    grams_list_record.append(ticket_df[\\'Top2Grams\\'][record])\\n    grams_list_record.append(ticket_df[\\'Top3Grams\\'][record])\\n    li=[]\\n#     li=[i for grams_words in grams_list_record for i in dict_keyword if fuzz.token_sort_ratio(grams_words,i)>80]\\n    for grams_words in grams_list_record:\\n        \\n        highest = process.extractOne(grams_words,dict_keyword.keys()  , scorer=fuzz.token_set_ratio)\\n        #scorer=fuzz.QRatio ,  scorer=fuzz.ratio\\n        li.append(highest[0])\\n#         for dict_words in dict_keyword:\\n#             if fuzz.token_sort_ratio(grams_words,dict_words)>80:\\n#                 li.append(dict_words)\\n#                 break\\n    \\n    n = {k: dict_keyword[k] for k in li if k in dict_keyword}\\n#     print(grams_list_record)\\n#     print(n)\\n    print(record)\\n    flag=bool(n)\\n    res=\\'\\'\\n    if flag==True:\\n        res=max(n, key=n.get)\\n    else:\\n        res=\\'\\'\\n    #print(res)\\n    automation_col.append(res)\\n\\n# In[54]:\\n\\nlen(automation_col)\\n\\n\\n# In[55]:\\n\\nautomation_col\\n\\n\\n# In[56]:\\n\\nautomation_col[2555]\\n\\n\\n# In[57]:\\n\\nlen(dict_automation_text)\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[58]:\\n\\n# final_values = {k: dict_automation_text[k] for k in automation_col if k in dict_automation_text }\\nautomation_text = [dict_automation_text[k] for k in automation_col if k in dict_automation_text]\\n# automation_text=list(final_values.values())\\n\\n\\n# In[59]:\\n\\nticket_df[\\'Automation Category\\']=pd.DataFrame({\\'Automation Category\\':automation_text})\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n# In[60]:\\n\\nticket_df.to_csv(\\'ticket_df_with_topgrams_automation_category_50K.csv\\')'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'not'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import collections\n",
    "from past.builtins import xrange\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#pointing the source and listing the file names under that path\n",
    "path = os.chdir('C://Users//ar393556//Documents//Ticket grams//sample')\n",
    "files = os.listdir(path)\n",
    "files\n",
    "\n",
    "\n",
    "#Fetching only xlsx files\n",
    "files_xls = [f for f in files if f[-4:] == 'xlsx']\n",
    "files_xls\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#Reading the files and appending the data in the DataFrame format\n",
    "for f in files_xls:\n",
    "    data = pd.read_excel(f,sheet_name='CI-Data')\n",
    "    df = df.append(data)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#Taking only TicketID and Summary which are essential\n",
    "\n",
    "ticket_df_full = pd.DataFrame(df, columns= ['TicketID','Summary'])\n",
    "\n",
    "ticket_df=ticket_df_full[['TicketID','Summary']]\n",
    "\n",
    "#no. of records and columns\n",
    "ticket_df.shape\n",
    "\n",
    "#Dropping null/blank records\n",
    "ticket_df=ticket_df.dropna()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "#New table data shape\n",
    "ticket_df.shape\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "ticket_df.head()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "ticket_df['unclean']=ticket_df['Summary']\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "ticket_df.head()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "# To remove punchuation, numbers and converting the string to lower\n",
    "ticket_df['Summary'] = ticket_df['Summary'].str.lower().str.replace(r'[^a-z\\s]', '')\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "ticket_df.head()    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "nlp.Defaults.stop_words.add('thanks')\n",
    "nlp.Defaults.stop_words.add('please')\n",
    "nlp.Defaults.stop_words.add('team')\n",
    "nlp.Defaults.stop_words.add('dear')\n",
    "nlp.Defaults.stop_words.add('hi')\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "nlp.Defaults.stop_words.remove('not')\n",
    "nlp.Defaults.stop_words.remove('cannot')\n",
    "nlp.Defaults.stop_words.remove('nothing')\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "stop_words=nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "\n",
    "ticket_df['Summary'] = ticket_df['Summary'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "ticket_df['Summary'] = ticket_df['Summary'].apply(lambda x: \" \".join(x for x in x.split() if len(x)>=2 and len(x)<15))\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "ticket_df['Summary']= ticket_df['Summary'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
    "# In[19]:\n",
    "\n",
    "ticket_df.head()\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "# ticket_df.to_csv('final words.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "ui=collections.Counter()\n",
    "\n",
    "for i in ticket_df['Summary']:\n",
    "    x = i.rstrip().split(\" \")\n",
    "    ui.update(ngrams(x, n=1))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "ui\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "ui_counter = pd.DataFrame.from_dict(ui, orient='index')\n",
    "ui_counter.to_csv('uigrams.csv')\n",
    "ui_df=pd.read_csv('uigrams.csv',index_col=False)\n",
    "ui_df['Unnamed: 0'] = ui_df['Unnamed: 0'].str.lower().str.replace(r'[^a-z\\s]', '')\n",
    "dict_ui=ui_df.set_index('Unnamed: 0')['0'].to_dict()\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "dict_ui\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "bi=collections.Counter()\n",
    "for i in ticket_df['Summary']:\n",
    "    x = i.rstrip().split(\" \")\n",
    "    bi.update(set(zip(x[:-1],x[1:])))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "bigram_counter = pd.DataFrame.from_dict(bi, orient='index')\n",
    "bigram_counter.to_csv('bigrams.csv')\n",
    "bigram_df=pd.read_csv('bigrams.csv',index_col=False)\n",
    "bigram_df['Unnamed: 0'] = bigram_df['Unnamed: 0'].str.lower().str.replace(r'[^a-z\\s]', '')\n",
    "dict_bigram=bigram_df.set_index('Unnamed: 0')['0'].to_dict()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "tri=collections.Counter()\n",
    "for i in ticket_df['Summary']:\n",
    "    x = i.rstrip().split(\" \")\n",
    "    tri.update(set(zip(x[:-2],x[1:-1],x[2:])))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "trigram_counter = pd.DataFrame.from_dict(tri, orient='index')\n",
    "trigram_counter.to_csv('trigram.csv')\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "trigram_df=pd.read_csv('trigram.csv',index_col=False)\n",
    "trigram_df['Unnamed: 0'] = trigram_df['Unnamed: 0'].str.lower().str.replace(r'[^a-z\\s]', '')\n",
    "dict_trigram=trigram_df.set_index('Unnamed: 0')['0'].to_dict()\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "four=collections.Counter()\n",
    "\n",
    "for i in ticket_df['Summary']:\n",
    "    x = i.rstrip().split(\" \")\n",
    "    four.update(ngrams(x, n=4))\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "fourgram_counter = pd.DataFrame.from_dict(four, orient='index')\n",
    "fourgram_counter.to_csv('fourgram.csv')\n",
    "fourgram_df=pd.read_csv('fourgram.csv',index_col=False)\n",
    "fourgram_df['Unnamed: 0'] = fourgram_df['Unnamed: 0'].str.lower().str.replace(r'[^a-z\\s]', '')\n",
    "dict_four=fourgram_df.set_index('Unnamed: 0')['0'].to_dict()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "def top_grams(dicti, N):\n",
    "    res_topgram=[]\n",
    "    for text in ticket_df['Summary']:  \n",
    "\n",
    "        comp_list=get_ngrams(text,N)\n",
    "        n = {k: dicti[k] for k in comp_list if k in dicti}\n",
    "        flag=bool(n)\n",
    "        res=''\n",
    "        if flag==True:\n",
    "            res=max(n, key=n.get)\n",
    "        else:\n",
    "            res='No Pattern'\n",
    "        res_topgram.append(res)\n",
    "        \n",
    "    return res_topgram\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "dict_ui\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "res_bigram=top_grams(dict_ui,1)\n",
    "ticket_df['Top1Grams']=pd.DataFrame({'Top1Grams':res_bigram})\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "res_bigram\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "res_bigram=top_grams(dict_bigram,2)\n",
    "ticket_df['Top2Grams']=pd.DataFrame({'Top2Grams':res_bigram})\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "ticket_df.head()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "res_trigram=top_grams(dict_trigram, 3)\n",
    "ticket_df['Top3Grams']=pd.DataFrame({'Top3Grams':res_trigram})\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "ticket_df.head()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "res_fourgram=top_grams(dict_four,4)\n",
    "ticket_df['Top4Grams']=pd.DataFrame({'Top4Grams':res_fourgram})\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "ticket_df.tail()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "ticket_df.to_csv('ticket_df_with_topgrams.csv')\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "path = os.chdir('C://Users//ar393556//Documents//Ticket grams//CIKeyword')\n",
    "files = os.listdir(path)\n",
    "\n",
    "automation_df=pd.read_excel('CIKeywords.xlsx')\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "automation_df.head()\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "automation_df['Keyword']=automation_df['Keyword'].apply(lambda x: x.strip(punctuation))\n",
    "automation_df['Keyword']=automation_df['Keyword'].str.replace('*',' ')\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "# To remove punchuation, numbers and converting the string to lower\n",
    "automation_df['Keyword']=automation_df['Keyword'].str.lower()\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "automation_df['Keyword']=automation_df['Keyword'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "automation_df.head()\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "dict_keyword=automation_df.set_index('Keyword')['Weight'].to_dict()\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "dict_automation_text=automation_df.set_index('Keyword')['AutomationCategory'].to_dict()\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "len(ticket_df)\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "automation_col=[]\n",
    "# for i in columns:\n",
    "for record in range(0,len(ticket_df)):\n",
    "    \n",
    "    grams_list_record=[]\n",
    "    grams_list_record.append(ticket_df['Top1Grams'][record])\n",
    "    grams_list_record.append(ticket_df['Top2Grams'][record])\n",
    "    grams_list_record.append(ticket_df['Top3Grams'][record])\n",
    "    li=[]\n",
    "#     li=[i for grams_words in grams_list_record for i in dict_keyword if fuzz.token_sort_ratio(grams_words,i)>80]\n",
    "    for grams_words in grams_list_record:\n",
    "        \n",
    "        highest = process.extractOne(grams_words,dict_keyword.keys()  , scorer=fuzz.token_set_ratio)\n",
    "        #scorer=fuzz.QRatio ,  scorer=fuzz.ratio\n",
    "        li.append(highest[0])\n",
    "#         for dict_words in dict_keyword:\n",
    "#             if fuzz.token_sort_ratio(grams_words,dict_words)>80:\n",
    "#                 li.append(dict_words)\n",
    "#                 break\n",
    "    \n",
    "    n = {k: dict_keyword[k] for k in li if k in dict_keyword}\n",
    "#     print(grams_list_record)\n",
    "#     print(n)\n",
    "    print(record)\n",
    "    flag=bool(n)\n",
    "    res=''\n",
    "    if flag==True:\n",
    "        res=max(n, key=n.get)\n",
    "    else:\n",
    "        res=''\n",
    "    #print(res)\n",
    "    automation_col.append(res)\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "len(automation_col)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "automation_col\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "automation_col[2555]\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "len(dict_automation_text)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "# final_values = {k: dict_automation_text[k] for k in automation_col if k in dict_automation_text }\n",
    "automation_text = [dict_automation_text[k] for k in automation_col if k in dict_automation_text]\n",
    "# automation_text=list(final_values.values())\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "ticket_df['Automation Category']=pd.DataFrame({'Automation Category':automation_text})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "ticket_df.to_csv('ticket_df_with_topgrams_automation_category_50K.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
